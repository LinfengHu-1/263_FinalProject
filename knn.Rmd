---
title: "project_corrmat"
author: "Cindy Lin"
date: "2023-04-27"
output: html_document
---

# YH SETUP 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(dplyr)
library(class)
library(pROC)
```

```{r}
library(dplyr)
dat <- read.csv('brfss_final.csv')
outcome <- data.frame(dat$X,dat$MICHD,dat$CVDINFR4,dat$CVDCRHD4)
outcome %>% group_by(dat.MICHD) %>% summarise(count=n())
outcome %>% group_by(dat.CVDINFR4) %>% summarise(count=n())
outcome %>% group_by(dat.CVDCRHD4) %>% summarise(count=n())

## remove the ones that responded don't know & not sure in CVDINFR4 & CVDCRHD4
dat <- dat[-which(dat$CVDINFR4 == 7 | dat$CVDINFR4 == 9),] 
dat <- dat[-which(dat$CVDCRHD4 == 7 | dat$CVDCRHD4 == 9),] 

# remove columns that has only 1 value for all rows
dat <- dat[ , -which(names(dat) %in% c("MEDSHEPB","TOLDCFS", "HAVECFS", "WORKCFS"))]
```

### Drop columns with more than 5% data missing, impute the rest using KNN
```{r}
# convert outcome variables
dat$MICHD <- factor(2-dat$MICHD)
dat$CVDINFR4 <- factor(2-dat$CVDINFR4)
dat$CVDCRHD4 <- factor(2-dat$CVDCRHD4)

# i believe X is the index column, not needed
# remove weights
dat <- dat[, !colnames(dat) %in% c('X', 'LLCPWT2', 'LLCPWT', 'CLLCPWT','STRWT','WT2RAKE')]
dat <- dat[, !colnames(dat) %in% c('QSTVER', 'STSTR','RAWRAKE')] # remove based on knowledge
threshold <- .05
ncol(dat) # 190

dat <- dat[, colMeans(is.na(dat)) <= threshold]
ncol(dat) # 52 columns left

columns_to_impute <- colnames(dat)[colSums(is.na(dat)) > 0]
columns_to_impute
str(dat[,columns_to_impute])

complete_columns <- colnames(dat)[colSums(is.na(dat)) == 0 & 
                                      !colnames(dat) %in% c('MICHD', 'CVDINFR4','CVDCRHD4')]

for (c in columns_to_impute) {
    col <- dat[[c]]
    scaled <- scale(dat[, complete_columns])
    knn <- knn(
        train = scaled[!is.na(col), complete_columns],
        test  = scaled[is.na(col), complete_columns], 
        cl    = dat[!is.na(col), c]
        )
    
    dat[is.na(col), c] = knn
}

colSums(is.na(dat))
```

```{r}
library(e1071)
library(caret)
library(ROCR)
set.seed(263)
train_index <- createDataPartition(dat$MICHD, p = 0.8, list = FALSE)
train <- dat[train_index, ]
test <- dat[-train_index, ]
```

## KNN

```{r knn baseline}

train$MICHD <- as.factor(train$MICHD)
test$MICHD <- as.factor(test$MICHD)

# baseline model fit for k = 1
knn_model <- knn(train = train,
                      test = test,
                      cl = train$MICHD,
                      k = 1)

# compute accuracy and test error 
#test_err <- mean(knn_model != test$MICHD)
accuracy <- sum(knn_model == test$MICHD)/length(test$MICHD)
sensitivity_knn <- sensitivity(knn_model, test$MICHD)
specificity_knn <- specificity(knn_model, test$MICHD)

print("Test Error and Accuracy for Baseline Model with K = 1")
print(paste0("Test Error: ", 1 - accuracy))
print(paste0("Accuracy: ", accuracy))
print(paste0("Sensitivity: ", sensitivity_knn))
print(paste0("Specificity: ", specificity_knn))

# confusion matrix - should we all include cm, and maybe aucroc? 
cm <- confusionMatrix(knn_model, test$MICHD)
cm


```

```{r knn tuning}

#using 10-fold cross validation to find the optimal k value, since we have around 20K data points
# RUN ONLY IF NEEDED - takes a long time, already got optimal k to be k = 27

# control <- trainControl(method='repeatedcv',
#                         number = 10,
#                         verboseIter = 20
#                         )
# 
# set.seed(52)
# knn_fit <- train(MICHD ~ .,
#                  method = 'knn',
#                  tuneGrid = expand.grid(k = 1:200),
#                  trControl = control,
#                  metric = "Accuracy",
#                  data = train)
# 
# knn_fit

```


```{r best knn}
# get metrics of knn using the best k-value  
# best k = 18
knn_model <- knn(train = train,
                      test = test,
                      cl = train$MICHD,
                      k = 27)

# compute accuracy and test error 
#test_err <- mean(knn_model != test$MICHD)
accuracy <- sum(knn_model == test$MICHD)/length(test$MICHD)
sensitivity_knn <- sensitivity(knn_model, test$MICHD)
specificity_knn <- specificity(knn_model, test$MICHD)
auc_score <- auc(roc(test$MICHD, as.numeric(knn_model)))
roc_curve <- roc(test$MICHD, as.numeric(knn_model))

print("Test Error and Accuracy for K=27")
print(paste0("Test Error: ", 1 - accuracy))
print(paste0("Accuracy: ", accuracy))
print(paste0("Sensitivity: ", sensitivity_knn))
print(paste0("Specificity: ", specificity_knn))
print(paste0("AUC Score: ", auc_score))


# confusion matrix - should we all include cm, and maybe aucroc? 
cm <- confusionMatrix(knn_model, test$MICHD)
cm

```


### KNN with PCA

```{r pca}
dat2 <- dat %>% select(-c("CVDINFR4", "CVDCRHD4", "MICHD"))
dat2 <- sapply(dat2, function (x) as.numeric(x))
pca <- prcomp(dat2, scale.=TRUE)

summary(pca)

pca30 <- pca$x[,1:30]
pca30

pca30_df <- data.frame(pca30, MICHD=dat$MICHD)
pca30_df

set.seed(263)
train_index <- createDataPartition(pca30_df$MICHD, p = 0.8, list = FALSE)
train <- pca30_df[train_index, ]
test <- pca30_df[-train_index, ]
```


```{r knnpc}

knn_model <- knn(train = train,
                      test = test,
                      cl = train$MICHD,
                      k = 1)

cm <- confusionMatrix(knn_model, test$MICHD)
cm
```

```{r knnpc tuning}

# control <- trainControl(method='repeatedcv',
#                         number = 10,
#                         verboseIter = 20
#                         )
# 
# set.seed(52)
# knn_fit <- train(MICHD ~ .,
#                  method = 'knn',
#                  tuneGrid = expand.grid(k = 1:200),
#                  trControl = control,
#                  metric = "Accuracy",
#                  data = train)
# 
# knn_fit

```

```{r knn best}
knn_model <- knn(train = train,
                      test = test,
                      cl = train$MICHD,
                      k = 198)

accuracy <- sum(knn_model == test$MICHD)/length(test$MICHD)
sensitivity_knn <- sensitivity(knn_model, test$MICHD)
specificity_knn <- specificity(knn_model, test$MICHD)
auc_score <- auc(roc(test$MICHD, as.numeric(knn_model)))
roc_curve <- roc(test$MICHD, as.numeric(knn_model))

print("Test Error and Accuracy for K=198")
print(paste0("Test Error: ", 1 - accuracy))
print(paste0("Accuracy: ", accuracy))
print(paste0("Sensitivity: ", sensitivity_knn))
print(paste0("Specificity: ", specificity_knn))
print(paste0("AUC Score: ", auc_score))

cm <- confusionMatrix(knn_model, test$MICHD)
cm



```

```{r random plot}

ggplot(pca30_df, aes(x=PC1, y=PC7, col=MICHD)) + geom_point() + ggtitle("PC1 v PC2")


```







