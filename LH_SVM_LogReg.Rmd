---
title: "SVM & Logistic Regression"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(dplyr)
library(class)
```

```{r}
library(dplyr)
dat <- read.csv('brfss_final.csv')
outcome <- data.frame(dat$X,dat$MICHD,dat$CVDINFR4,dat$CVDCRHD4)
outcome %>% group_by(dat.MICHD) %>% summarise(count=n())
outcome %>% group_by(dat.CVDINFR4) %>% summarise(count=n())
outcome %>% group_by(dat.CVDCRHD4) %>% summarise(count=n())

## remove the ones that responded don't know & not sure in CVDINFR4 & CVDCRHD4
dat <- dat[-which(dat$CVDINFR4 == 7 | dat$CVDINFR4 == 9),] 
dat <- dat[-which(dat$CVDCRHD4 == 7 | dat$CVDCRHD4 == 9),] 

# remove columns that has only 1 value for all rows
dat <- dat[ , -which(names(dat) %in% c("MEDSHEPB","TOLDCFS", "HAVECFS", "WORKCFS"))]
```

### Drop columns with more than 5% data missing, impute the rest using KNN
```{r}
# convert outcome variables
dat$MICHD <- factor(2-dat$MICHD)
dat$CVDINFR4 <- factor(2-dat$CVDINFR4)
dat$CVDCRHD4 <- factor(2-dat$CVDCRHD4)

# i believe X is the index column, not needed
# remove weights
dat <- dat[, !colnames(dat) %in% c('X', 'LLCPWT2', 'LLCPWT', 'CLLCPWT','STRWT','WT2RAKE')]
dat <- dat[, !colnames(dat) %in% c('QSTVER', 'STSTR','RAWRAKE')] # remove based on knowledge
threshold <- .05
ncol(dat) # 190

dat <- dat[, colMeans(is.na(dat)) <= threshold]
ncol(dat) # 52 columns left

columns_to_impute <- colnames(dat)[colSums(is.na(dat)) > 0]
#columns_to_impute
str(dat[,columns_to_impute])

complete_columns <- colnames(dat)[colSums(is.na(dat)) == 0 & 
                                      !colnames(dat) %in% c('MICHD', 'CVDINFR4','CVDCRHD4')]

for (c in columns_to_impute) {
    col <- dat[[c]]
    scaled <- scale(dat[, complete_columns])
    knn <- knn(
        train = scaled[!is.na(col), complete_columns],
        test  = scaled[is.na(col), complete_columns], 
        cl    = dat[!is.na(col), c]
        )
    
    dat[is.na(col), c] = knn
}

colSums(is.na(dat))
```


### Start modeling

```{r}
library(e1071)
library(caret)
library(ROCR)
library(pROC)
set.seed(263)
dat <- dat[, !(colnames(dat) %in% c("CVDINFR4","CVDCRHD4"))]
train_index <- createDataPartition(dat$MICHD, p = 0.8, list = FALSE)
train <- dat[train_index, ]
test <- dat[-train_index, ]
```


### PCA

```{r}
train_data <- train[, !(colnames(train) %in% c("MICHD"))]
test_data <- test[, !(colnames(test) %in% c("MICHD"))]

train_pc <- apply(train_data, 2, as.numeric)
pca <- prcomp(train_pc, scale. = TRUE)  
pca_data <- predict(pca, newdata = train_pc)[, 1:30]  
train_pc <- data.frame(pca_data, MICHD = train$MICHD)
test_pc <- apply(test_data, 2, as.numeric)
pca <- prcomp(test_pc, scale. = TRUE)  
pca_data <- predict(pca, newdata = test_pc)[, 1:30]  
test_pc <- data.frame(pca_data, MICHD = test$MICHD)

#logit_model <- glm(MICHD ~ ., data = pca_dat, family = binomial(link = "logit"))
train_pc$MICHD <- make.names(train_pc$MICHD)
ctrl <- trainControl(method = "cv",number = 10,classProbs = TRUE,
                     summaryFunction = twoClassSummary, savePredictions = TRUE)
fit_michd <- train(MICHD ~ ., data = train_pc, method = "glm", 
                   family = "binomial", trControl = ctrl, metric = "ROC")
result_MICHD <- data.frame(fit_michd$results)
result_MICHD <- result_MICHD[,2:4]
result_MICHD <- cbind(Model = "Plain Logistic Regression", Outcome = "MICHD", result_MICHD)
result_MICHD

test_pc$MICHD <- make.names(test_pc$MICHD)
log_pred <- predict(fit_michd, newdata = test_pc)
cm <- confusionMatrix(data = log_pred, reference = factor(test_pc$MICHD))
cm

# Extract the prediction values as numeric
log_pred <- as.numeric(predict(fit_michd, newdata = test_pc))
roc_data <- data.frame(actual = test_pc$MICHD, predicted = log_pred)
roc_log_pc <- roc(roc_data$actual, roc_data$predicted)
#ggplot(data = data.frame(x = 1 - roc$specificities, y = roc$sensitivities)) +
#  geom_line(aes(x = x, y = y)) +
#  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
#  labs(title = "Receiver Operating Characteristic (ROC) Curve",
#       x = "False Positive Rate (1 - Specificity)",
#       y = "True Positive Rate (Sensitivity)")
plot(roc_log_pc, col = "blue", main = "ROC curves", print.auc = TRUE, auc.polygon = TRUE, legacy.axes = TRUE)
```






## Linfeng - Plain Logistic Regression
```{r}
train$MICHD <- make.names(train$MICHD)
#train$CVDCRHD4 <- make.names(train$CVDCRHD4)
#train$CVDINFR4 <- make.names(train$CVDINFR4)
test$MICHD <- make.names(test$MICHD)
#test$CVDCRHD4 <- make.names(test$CVDCRHD4)
#test$CVDINFR4 <- make.names(test$CVDINFR4)
#mylogit <- glm(MICHD ~ .- CVDINFR4 - CVDCRHD4,data=train, na.action = na.omit, family="binomial")
library(caret)
ctrl <- trainControl(method = "cv",number = 10,classProbs = TRUE,
                     summaryFunction = twoClassSummary, savePredictions = TRUE)
fit_michd <- train(MICHD ~ ., data = train, method = "glm", 
                   family = "binomial", trControl = ctrl, metric = "ROC")
result_MICHD <- data.frame(fit_michd$results)
result_MICHD <- result_MICHD[,2:4]
result_MICHD <- cbind(Model = "Plain Logistic Regression", Outcome = "MICHD", result_MICHD)
# fit_mi <- train(CVDINFR4 ~ .- MICHD - CVDCRHD4, data = train, method = "glm", 
#                    family = "binomial", trControl = ctrl, metric = "ROC")
# result_MI <- data.frame(fit_mi$results)
# result_MI <- result_MI[,2:4]
# result_MI <- cbind(Model = "Plain Logistic Regression", Outcome = "MI", result_MI)
# fit_chd <- train(CVDCRHD4 ~ .- CVDINFR4 - MICHD, data = train, method = "glm", 
#                    family = "binomial", trControl = ctrl, metric = "ROC")
# result_CHD <- data.frame(fit_chd$results)
# result_CHD <- result_CHD[,2:4]
# result_CHD <- cbind(Model = "Plain Logistic Regression", Outcome = "CHD", result_CHD)
# 
# result_log <- rbind(result_MI, result_CHD, result_MICHD)
# result_log

log_pred <- predict(fit_michd, newdata = test)
cm <- confusionMatrix(data = log_pred, reference = factor(test$MICHD))
cm
log_pred <- as.numeric(predict(fit_michd, newdata = test))
roc_data <- data.frame(actual = test$MICHD, predicted = log_pred)
library(pROC)
roc_log_normal <- roc(roc_data$actual, roc_data$predicted)
ggplot(data = data.frame(x = 1 - roc$specificities, y = roc$sensitivities)) +
  geom_line(aes(x = x, y = y)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(title = "Receiver Operating Characteristic (ROC) Curve",
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)")
```



## Ridge & Lasso for Feature Selection

```{r}
x <- model.matrix(MICHD~., data=train)
y <- train$MICHD
grid <- seq(0, .8, length=100)
cv_lasso <- cv.glmnet(x, as.numeric(y), alpha=1, lambda=grid, penalty.factor = c(rep(1,18), rep(0,ncol(x)-18)))

print(cv_lasso)
plot(cv_lasso)
```


## Linfeng - SVM

## SVM with PCA
```{r}
ctrl <- trainControl(method = "cv", number = 3, summaryFunction = twoClassSummary, 
                     classProbs = TRUE, verboseIter = TRUE)
tuneGrid <- expand.grid(C = c(0.1, 1, 10), sigma = c(0.1, 1, 10))
svm_model <- train(MICHD ~ ., data = train_pc, method = "svmRadial", trControl = ctrl, tuneGrid = tuneGrid, metric = "ROC")
#sigma = 0.1, C = 10
svmlin_pred <- predict(svm_model, newdata = test_pc)
cm <- confusionMatrix(data = svmlin_pred, reference = factor(test_pc$MICHD))
cm
svmlin_pred <- as.numeric(predict(svm_model, newdata = test_pc))
roc_data <- data.frame(actual = test_pc$MICHD, predicted = svmlin_pred)
library(pROC)
roc <- roc(roc_data$actual, roc_data$predicted)
ggplot(data = data.frame(x = 1 - roc$specificities, y = roc$sensitivities)) +
  geom_line(aes(x = x, y = y)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(title = "Receiver Operating Characteristic (ROC) Curve",
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)")
############################ Linear ############################
SVMlinearGrid <- expand.grid(C = c(0.1, 0.7, 1, 10))
svm_linear <- train(MICHD ~ ., data = train_pc, method = "svmLinear", trControl = ctrl, tuneGrid = SVMlinearGrid, metric = "ROC")
result_linear_MICHD <- data.frame(svm_linear$results)
result_linear_MICHD <- result_linear_MICHD[,1:4]
result_linear_MICHD <- cbind(Model = "SVM - Linear", Outcome = "MICHD", result_linear_MICHD)
result_linear_MICHD
svm_linear$bestTune #  C = 0.7
svm_pred <- predict(svm_linear, newdata = test_pc)
cm <- confusionMatrix(data = svm_pred, reference = factor(test_pc$MICHD))
cm
svm_pred <- as.numeric(svm_pred)
roc_data <- data.frame(actual = test_pc$MICHD, predicted = svm_pred)
roc <- roc(roc_data$actual, roc_data$predicted)
ggplot(data = data.frame(x = 1 - roc$specificities, y = roc$sensitivities)) +
  geom_line(aes(x = x, y = y)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(title = "Receiver Operating Characteristic (ROC) Curve",
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)")
############################ Polynomial ############################
svm_poly <- train(MICHD ~ ., data = train_pc, method = "svmPoly", trControl = ctrl, tuneLength = 3, preProcess = c("center","scale"),metric = "ROC")
result_poly_MICHD <- data.frame(svm_poly$results)
result_poly_MICHD <- result_poly_MICHD[,1:6]
result_poly_MICHD <- cbind(Model = "SVM - Polynomial", Outcome = "MICHD", result_poly_MICHD)
result_poly_MICHD
svm_poly$bestTune # degree = 3, scale = 0.01, C = 1
# Predict on test dataset
# Construct the SVM with the best hyperparameters
svm_poly <- svm(as.numeric(factor(train_pc$MICHD)) ~ ., data = train_pc, kernel = "polynomial", 
                degree = 3, gamma = 1/0.01, cost = 1)
predictions <- predict(svm_poly, test_pc)


# Predict using the SVM on the test data
predictions = svm_poly.predict(test_pc)
svm_pred <- predict(svm_poly, newdata = test_pc)
# Calculate accuracy
cm <- confusionMatrix(data = svm_pred, reference = factor(test$MICHD_pc))
cm
svm_pred <- as.numeric(svm_pred)
roc_data <- data.frame(actual = test_pc$MICHD, predicted = svm_pred)
roc <- roc(roc_data$actual, roc_data$predicted)
ggplot(data = data.frame(x = 1 - roc$specificities, y = roc$sensitivities)) +
  geom_line(aes(x = x, y = y)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(title = "Receiver Operating Characteristic (ROC) Curve",
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)")
```



## plain SVM
```{r}
ctrl <- trainControl(method = "cv", number = 3, summaryFunction = twoClassSummary, 
                     classProbs = TRUE, verboseIter = TRUE)
tuneGrid <- expand.grid(C = c(0.1, 1, 10), sigma = c(0.1, 1, 10))
svm_model <- train(MICHD ~ ., data = train, method = "svmRadial", trControl = ctrl, tuneGrid = tuneGrid, metric = "ROC")
result_MICHD <- data.frame(svm_model$results)
result_MICHD <- result_MICHD[,1:5]
result_MICHD <- cbind(Model = "SVM - Radial", Outcome = "MICHD", result_MICHD)
result_MICHD
svm_model$bestTune # C = 1, sigma = 0.1
# get Confusion Matrix
train$MICHD <- as.factor(train$MICHD)
svm_model <- svm(MICHD ~ ., data = train, cost = 1, sigma = 0.1,kernel = "radial")
# Make predictions on the test set
svm_pred <- predict(svm_model, newdata = test)
accuracy <- sum(svm_pred == test$MICHD) / nrow(test)

# Linear Kernel
SVMlinearGrid <- expand.grid(C = c(0.1, 0.7, 1, 10))
svm_linear <- train(MICHD ~ ., data = train, method = "svmLinear", trControl = ctrl, tuneGrid = SVMlinearGrid, metric = "ROC")
result_linear_MICHD <- data.frame(svm_linear$results)
result_linear_MICHD <- result_linear_MICHD[,1:4]
result_linear_MICHD <- cbind(Model = "SVM - Linear", Outcome = "MICHD", result_linear_MICHD)
result_linear_MICHD
svm_linear$bestTune
svm_pred <- predict(svm_linear, newdata = test)
# Calculate accuracy
cm <- confusionMatrix(data = svm_pred, reference = factor(test$MICHD))
cm
library(pROC)
svm_pred <- predict(svm_linear, newdata = test, type = "prob")
svm_auc <- roc(factor(test$MICHD), svm_pred[,"X1"])$auc
svm_auc


# Fit the model 
svm_poly <- train(MICHD ~ .-CVDINFR4-CVDCRHD4, data = train, method = "svmPoly", trControl = ctrl, tuneLength = 3, preProcess = c("center","scale"),metric = "ROC")
result_poly_MICHD <- data.frame(svm_poly$results)
result_poly_MICHD <- result_poly_MICHD[,1:6]
result_poly_MICHD <- cbind(Model = "SVM - Polynomial", Outcome = "MICHD", result_poly_MICHD)
result_poly_MICHD
svm_poly$bestTune # degree = 2, scale = 0.01, C = 0.25
# Predict on test dataset
svm_pred <- predict(svm_poly, newdata = test)
# Calculate accuracy
cm <- confusionMatrix(data = svm_pred, reference = factor(test$MICHD))
cm
library(pROC)
svm_pred <- predict(svm_poly, newdata = test, type = "prob")
svm_auc <- roc(factor(test$MICHD), svm_pred[,"X1"])$auc
svm_auc
```

## ROC plots

Plain logistic regression
```{r}
#plot(roc_log_pc, col = "blue", main = "ROC curves", print.auc = TRUE, 
#     auc.polygon = TRUE, legacy.axes = TRUE, xlim = c(1.2, -0.2))
#plot(roc_log_normal, col = "red", add = TRUE, print.auc = TRUE, 
#     auc.polygon = TRUE, legacy.axes = TRUE)
#legend("bottomright", legend = c("Plain Logistic Regression - PCA", "Plain Logistic Regression"),
#       col = c("blue", "red"), lty = 1, cex = 0.6)
library(ggplot2)
library(scales)
# Create data frames with the ROC curve data for each model
roc_pc_df <- data.frame(fpr = roc_log_pc$specificities, tpr = roc_log_pc$sensitivities)
roc_normal_df <- data.frame(fpr = roc_log_normal$specificities, tpr = roc_log_normal$sensitivities)
# Create the combined ROC plot
ggplot() +
  geom_line(data = roc_pc_df, aes(x = fpr, y = tpr, color = "Plain Logistic Regression - PCA")) +
  geom_line(data = roc_normal_df, aes(x = fpr, y = tpr, color = "Plain Logistic Regression")) +
  scale_color_manual(name = "Models", values = c("blue", "red")) +
  labs(x = "False Positive Rate", y = "True Positive Rate", 
       title = "ROC curves for Plain Logistic Regression") +
  xlim(1,0)+
  theme_classic() +
  theme(legend.position = "bottom")
```

SVM
```{r}
#PCA: Radial - sigma = 0.1, C = 10; Linear - C = 0.7; Polynomial - degree = 3, scale = 0.01, C = 1
svm_rbf_pca <- svm(as.numeric(factor(MICHD)) ~ ., 
                   data = train_pc, kernel = "radial", gamma = 0.1, cost = 10)
svm_linear_pca <- svm(as.numeric(factor(MICHD)) ~ ., data = train_pc, kernel = "linear", cost = 0.7)
svm_poly_pca <- svm(as.numeric(factor(MICHD)) ~ ., data = train_pc, kernel = "polynomial", 
                    degree = 3, cost = 1, scale = 0.01)
#normal: Radial - C = 1, sigma = 0.1; Linear - C = 10; Polynomial - degree = 2, scale = 0.01, C = 0.25
svm_rbf_normal <- svm(as.numeric(factor(MICHD)) ~ ., data = train, kernel = "radial", 
                      gamma = 0.1, cost = 1)
svm_linear_normal <- svm(as.numeric(factor(MICHD)) ~ ., data = train, kernel = "linear", cost = 10)
svm_poly_normal <- svm(as.numeric(factor(MICHD)) ~ ., data = train, kernel = "polynomial", 
                       degree = 2, cost = 0.25, scale = 0.01)


# Make predictions on the test set for each SVM model
test_pc <- predict(preProcess(test, method = c("center", "scale", "pca"))$pca, test)
pred_rbf_pca <- predict(svm_rbf_pca, test_pc)
pred_linear_pca <- predict(svm_linear_pca, test_pc)
pred_poly_pca <- predict(svm_poly_pca, test_pc)
pred_rbf_normal <- predict(svm_rbf_normal, test)
pred_linear_normal <- predict(svm_linear_normal, test)
pred_poly_normal <- predict(svm_poly_normal, test)

# Calculate the ROC curve and AUC for each SVM model
roc_rbf_pca <- roc(test_pc$MICHD, round(pred_rbf_pca))
roc_linear_pca <- roc(test_pc$MICHD, round(pred_linear_pca))
roc_poly_pca <- roc(test_pc$MICHD, round(pred_poly_pca))
roc_rbf_normal <- roc(test$MICHD, round(pred_rbf_normal))
roc_linear_normal <- roc(test$MICHD, round(pred_linear_normal))
roc_poly_normal <- roc(test$MICHD, round(pred_poly_normal))

roc_rbf_pca_df <- data.frame(fpr = roc_rbf_pca$specificities, tpr = roc_rbf_pca$sensitivities)
roc_linear_pca_df <- data.frame(fpr = roc_linear_pca$specificities, 
                                tpr = roc_linear_pca$sensitivities)
roc_poly_pca_df <- data.frame(fpr = roc_poly_pca$specificities, tpr = roc_poly_pca$sensitivities)
roc_rbf_normal_df <- data.frame(fpr = roc_rbf_normal$specificities, 
                                tpr = roc_rbf_normal$sensitivities)
roc_linear_normal_df <- data.frame(fpr = roc_linear_normal$specificities, 
                                   tpr = roc_linear_normal$sensitivities)
roc_poly_normal_df <- data.frame(fpr = roc_poly_normal$specificities, 
                                 tpr = roc_poly_normal$sensitivities)
# Create the combined ROC plot
ggplot() +
  geom_line(data = roc_rbf_pca_df, aes(x = fpr, y = tpr, color = "SVM(RBF) - PCA", linetype = "solid")) +
  geom_line(data = roc_linear_pca_df, aes(x = fpr, y = tpr, color = "SVM(Linear) - PCA", linetype = "solid")) +
  geom_line(data = roc_poly_pca_df, aes(x = fpr, y = tpr, color = "SVM(Polynomial) - PCA", linetype = "solid")) +
  geom_line(data = roc_rbf_normal_df, aes(x = fpr, y = tpr, color = "SVM(RBF)", linetype = "dashed")) +
  geom_line(data = roc_linear_normal_df, aes(x = fpr, y = tpr, color = "SVM(Linear)", linetype = "dashed")) +
  geom_line(data = roc_poly_normal_df, aes(x = fpr, y = tpr, color = "SVM(Polynomial)", linetype = "dashed")) +
  scale_color_manual(name = "Models", 
                     values = c("SVM(RBF) - PCA" = "red", 
                                "SVM(Linear) - PCA" = "blue", 
                                "SVM(Polynomial) - PCA" = "green", 
                                "SVM(RBF)" = "red", 
                                "SVM(Linear)" = "blue", 
                                "SVM(Polynomial)" = "green")) +
  scale_linetype_manual(name = "Models", 
                        values = c("solid", "dashed", "solid", "dashed", "solid", "dashed")) +
  labs(x = "False Positive Rate", y = "True Positive Rate", 
       title = "ROC curves for Support Vector Machine") +
  xlim(1,0) +
  theme_classic() +
  theme(legend.position = "bottom")
```


```{r}
ggplot() +
  geom_line(data = roc_rbf_pca_df, aes(x = fpr, y = tpr, color = "SVM(RBF) - PCA"),
            linetype = "solid", color = "red",show.legend = TRUE) +
  geom_line(data = roc_linear_pca_df, aes(x = fpr, y = tpr, color = "SVM(Linear) - PCA"),
            linetype = "solid", color = "blue",show.legend = TRUE) +
  geom_line(data = roc_poly_pca_df, aes(x = fpr, y = tpr, color = "SVM(Polynomial) - PCA"),
            linetype = "solid", color = "green",show.legend = TRUE) +
  geom_line(data = roc_rbf_normal_df, aes(x = fpr, y = tpr, color = "SVM(RBF)"),
            linetype = "dashed", color = "red",show.legend = TRUE) +
  geom_line(data = roc_linear_normal_df, aes(x = fpr, y = tpr, color = "SVM(Linear)"),
            linetype = "dashed", color = "blue",show.legend = TRUE) +
  geom_line(data = roc_poly_normal_df, aes(x = fpr, y = tpr, color = "SVM(Polynomial)"),
            linetype = "dashed", color = "green",show.legend = TRUE) +
  scale_color_manual(name = "Models", 
                     values = c("SVM(RBF) - PCA" = "red", 
                                "SVM(Linear) - PCA" = "blue", 
                                "SVM(Polynomial) - PCA" = "green", 
                                "SVM(RBF)" = "red", 
                                "SVM(Linear)" = "blue", 
                                "SVM(Polynomial)" = "green"),
                     labels = c("SVM(RBF) - PCA", "SVM(Linear) - PCA", "SVM(Polynomial) - PCA",
                                "SVM(RBF)", "SVM(Linear)", "SVM(Polynomial)")) +
  labs(x = "False Positive Rate", y = "True Positive Rate", 
       title = "ROC curves for Support Vector Machine") +
  xlim(1,0) +
  theme_classic() +
  theme(legend.position = "bottom")

```


```{r}
ggplot() +
  geom_line(data = roc_rbf_pca_df, aes(x = fpr, y = tpr, color = "SVM(RBF) - PCA", linetype = "solid")) +
  geom_line(data = roc_linear_pca_df, aes(x = fpr, y = tpr, color = "SVM(Linear) - PCA", linetype = "solid")) +
  geom_line(data = roc_poly_pca_df, aes(x = fpr, y = tpr, color = "SVM(Polynomial) - PCA", linetype = "solid")) +
  geom_line(data = roc_rbf_normal_df, aes(x = fpr, y = tpr, color = "SVM(RBF)", linetype = "dashed")) +
  geom_line(data = roc_linear_normal_df, aes(x = fpr, y = tpr, color = "SVM(Linear)", linetype = "dashed")) +
  geom_line(data = roc_poly_normal_df, aes(x = fpr, y = tpr, color = "SVM(Polynomial)", linetype = "dashed")) +
  scale_color_manual(name = "Models", 
                     values = c("SVM(RBF) - PCA" = "red", 
                                "SVM(Linear) - PCA" = "blue", 
                                "SVM(Polynomial) - PCA" = "green", 
                                "SVM(RBF)" = "red", 
                                "SVM(Linear)" = "blue", 
                                "SVM(Polynomial)" = "green")) +
  scale_linetype_manual(name = "Models", 
                        values = c("solid", "dashed", "solid", "dashed", "solid", "dashed")) +
  labs(x = "False Positive Rate", y = "True Positive Rate", 
       title = "ROC curves for Support Vector Machine") +
  xlim(1,0) +
  theme_classic() +
  theme(legend.position = "bottom")

```


```{r}
ggplot() +
  geom_line(data = roc_rbf_pca_df, aes(x = fpr, y = tpr, color = "SVM(RBF) - PCA"), linetype = "solid", size = 1) +
  geom_line(data = roc_linear_pca_df, aes(x = fpr, y = tpr, color = "SVM(Linear) - PCA"), linetype = "solid", size = 1) +
  geom_line(data = roc_poly_pca_df, aes(x = fpr, y = tpr, color = "SVM(Polynomial) - PCA"), linetype = "solid", size = 1) +
  geom_line(data = roc_rbf_normal_df, aes(x = fpr, y = tpr, color = "SVM(RBF)"), linetype = "dashed", size = 1) +
  geom_line(data = roc_linear_normal_df, aes(x = fpr, y = tpr, color = "SVM(Linear)"), linetype = "dashed", size = 1) +
  geom_line(data = roc_poly_normal_df, aes(x = fpr, y = tpr, color = "SVM(Polynomial)"), linetype = "dashed", size = 1) +
  scale_color_manual(name = "Models", 
                     values = c("SVM(RBF) - PCA" = "red", 
                                "SVM(Linear) - PCA" = "blue", 
                                "SVM(Polynomial) - PCA" = "green", 
                                "SVM(RBF)" = "red", 
                                "SVM(Linear)" = "blue", 
                                "SVM(Polynomial)" = "green"),
                     guide = guide_legend(override.aes = list(linetype = c("solid", "solid", "solid", "dashed", "dashed", "dashed")))) +
  labs(x = "False Positive Rate", y = "True Positive Rate", 
       title = "ROC curves for Support Vector Machine") +
  xlim(1,0)+
  theme_classic() +
  theme(legend.position = "bottom")

```








```{r}
ggplot() +
  geom_line(data = roc_rbf_normal_df, aes(x = fpr, y = tpr, color = "SVM(RBF)")) +
  geom_line(data = roc_linear_normal_df, aes(x = fpr, y = tpr, color = "SVM(Linear)")) +
  geom_line(data = roc_poly_normal_df, aes(x = fpr, y = tpr, color = "SVM(Polynomial)")) +
  scale_color_manual(name = "Models", 
                     values = c("SVM(RBF)" = "red", 
                                "SVM(Linear)" = "blue", 
                                "SVM(Polynomial)" = "green")) +
  labs(x = "False Positive Rate", y = "True Positive Rate", 
       title = "ROC curves for Support Vector Machine") +
  xlim(1,0) +
  theme_classic() +
  theme(legend.position = "bottom")
```









```{r}
################################## Ridge Regression ############################
# set up grid of lambda values to try
lambda_grid <- 10^seq(-4, 4, by = 1)
# perform cross-validation to tune lambda
cv_results <- cv.glmnet(x = model.matrix(MICHD ~ .- CVDINFR4 - CVDCRHD4, data = train),
                        y = train$MICHD,
                        alpha = 0,  # L2 penalty (ridge regression)
                        family = "binomial",
                        type.measure = "class",  # use classification accuracy as evaluation metric
                        lambda = lambda_grid)
# select lambda that yields best classification accuracy
best_lambda <- cv_results$lambda.min
# train model on full training set with selected lambda
mylogit <- glmnet(x = model.matrix(MICHD ~ .- CVDINFR4 - CVDCRHD4, data = train),
                  y = train$MICHD,
                  alpha = 0,  # L2 penalty (ridge regression)
                  family = "binomial",
                  lambda = best_lambda)
# evaluate model performance on test set
pred <- predict(mylogit, newx = model.matrix(MICHD ~ .- CVDINFR4 - CVDCRHD4, data = test))
pred_class <- ifelse(pred > 0, 1, 0)
accuracy <- mean(pred_class == test$MICHD)
sensitivity <- sum(pred_class[test$MICHD == 1] == 1) / sum(test$MICHD == 1)
specificity <- sum(pred_class[test$MICHD == 0] == 0) / sum(test$MICHD == 0)
```
