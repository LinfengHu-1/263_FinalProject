---
title: "random_forest_update"
author: "Linfeng Hu"
date: "2023-05-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(dplyr)
library(class)
library(e1071)
library(caret)
library(ROCR)
set.seed(263)
```

```{r}
library(dplyr)
dat <- read.csv('brfss_final.csv')
outcome <- data.frame(dat$X,dat$MICHD,dat$CVDINFR4,dat$CVDCRHD4)
outcome %>% group_by(dat.MICHD) %>% summarise(count=n())
outcome %>% group_by(dat.CVDINFR4) %>% summarise(count=n())
outcome %>% group_by(dat.CVDCRHD4) %>% summarise(count=n())

## remove the ones that responded don't know & not sure in CVDINFR4 & CVDCRHD4
dat <- dat[-which(dat$CVDINFR4 == 7 | dat$CVDINFR4 == 9),] 
dat <- dat[-which(dat$CVDCRHD4 == 7 | dat$CVDCRHD4 == 9),] 

# remove columns that has only 1 value for all rows
dat <- dat[ , -which(names(dat) %in% c("MEDSHEPB","TOLDCFS", "HAVECFS", "WORKCFS"))]
```

### Drop columns with more than 5% data missing, impute the rest using KNN
```{r}
# convert outcome variables
dat$MICHD <- factor(2-dat$MICHD)
dat$CVDINFR4 <- factor(2-dat$CVDINFR4)
dat$CVDCRHD4 <- factor(2-dat$CVDCRHD4)

# i believe X is the index column, not needed
# remove weights
dat <- dat[, !colnames(dat) %in% c('X', 'LLCPWT2', 'LLCPWT', 'CLLCPWT','STRWT','WT2RAKE')]
dat <- dat[, !colnames(dat) %in% c('QSTVER', 'STSTR','RAWRAKE')] # remove based on knowledge
threshold <- .05
ncol(dat) # 190

dat <- dat[, colMeans(is.na(dat)) <= threshold]
ncol(dat) # 52 columns left

columns_to_impute <- colnames(dat)[colSums(is.na(dat)) > 0]
columns_to_impute
str(dat[,columns_to_impute])

complete_columns <- colnames(dat)[colSums(is.na(dat)) == 0 & 
                                      !colnames(dat) %in% c('MICHD', 'CVDINFR4','CVDCRHD4')]

for (c in columns_to_impute) {
    col <- dat[[c]]
    scaled <- scale(dat[, complete_columns])
    knn <- knn(
        train = scaled[!is.na(col), complete_columns],
        test  = scaled[is.na(col), complete_columns], 
        cl    = dat[!is.na(col), c]
        )
    
    dat[is.na(col), c] = knn
}

colSums(is.na(dat))
```



```{r}
train_index <- createDataPartition(dat$MICHD, p = 0.8, list = FALSE)
train <- dat[train_index, ]
test <- dat[-train_index, ]
train$weights <- ifelse(as.numeric(train$MICHD) == 1,
                        1/mean(as.numeric(train$MICHD) == 1),
                        1/(1-mean(as.numeric(train$MICHD) == 1)))
train$weights <- as.numeric(train$weights)
test$weights <- ifelse(as.numeric(test$MICHD) == 1,
                        1/mean(as.numeric(test$MICHD) == 1),
                        1/(1-mean(as.numeric(test$MICHD) == 1)))
test$weights <- as.numeric(test$weights)
index_weight <- which(names(train) == "weights")
```

```{r}
index_michd <- which(names(train) == "MICHD")
index_infr <- which(names(train) == "CVDINFR4")
index_crhd <- which(names(train) == "CVDCRHD4")

index_weight_train <- which(names(train) == "weights")
index_weight_test <- which(names(test) == "weights")
```


```{r}
library(randomForest)
rf_best <- randomForest(as.factor(MICHD) ~.,
                        data = train[, -c(index_infr, index_crhd, index_weight_train)],
                        mtry = 6, ntree = 21, nodesize = 10, weights = train[, index_weight])
```

```{r}
pred_test <- predict(rf_best, newdata = test)
cm_test <- confusionMatrix(pred_test, as.factor(test$MICHD))
cm_test
```




