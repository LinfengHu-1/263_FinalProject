---
title: "BST 263 Final Project: XGBoost model"
author: "Bina Choi"
date: 2023.05.07
output:
  pdf_document: default
---
# Libraries and functions

```{r}
library(tidyverse)
library(dplyr)
library(janitor)
library(xgboost)
library(glmnet)
library(dplyr)
library(class)
library(caret)
library(e1071)
library(caret)
library(ROCR)
```

# Load in the data, imputation, and train/test split

```{r}
dat <- read.csv('brfss_final.csv')
outcome <- data.frame(dat$X,dat$MICHD,dat$CVDINFR4,dat$CVDCRHD4)
outcome %>% group_by(dat.MICHD) %>% summarise(count=n())
outcome %>% group_by(dat.CVDINFR4) %>% summarise(count=n())
outcome %>% group_by(dat.CVDCRHD4) %>% summarise(count=n())

## remove the ones that responded don't know & not sure in CVDINFR4 & CVDCRHD4
dat <- dat[-which(dat$CVDINFR4 == 7 | dat$CVDINFR4 == 9),] 
dat <- dat[-which(dat$CVDCRHD4 == 7 | dat$CVDCRHD4 == 9),] 

# remove columns that has only 1 value for all rows
dat <- dat[ , -which(names(dat) %in% c("MEDSHEPB","TOLDCFS", "HAVECFS", "WORKCFS"))]
```

### Drop columns with more than 5% data missing, impute the rest using KNN
```{r}
# convert outcome variables
dat$MICHD <- factor(2-dat$MICHD)
dat$CVDINFR4 <- factor(2-dat$CVDINFR4)
dat$CVDCRHD4 <- factor(2-dat$CVDCRHD4)

# i believe X is the index column, not needed
# remove weights
dat <- dat[, !colnames(dat) %in% c('X', 'LLCPWT2', 'LLCPWT', 'CLLCPWT','STRWT','WT2RAKE')]
threshold <- .05
ncol(dat) # 190

###
### Missing data
na_count <- data.frame(na = sapply(dat, function(y) sum(length(which(is.na(y))))))

na_count$variable <- rownames(na_count)
rownames(na_count) <- NULL

na_count %>%
  as_tibble() %>%
  select(variable, na) %>%
  filter(na != 0) %>% 
  mutate(na_percent = na/28433) %>% 
  arrange(desc(na)) 

dat %>%  
  mutate(na_rowise = rowSums(is.na(.))) %>% 
  ggplot(aes(na_rowise)) + geom_histogram(color = "black") + 
  labs(x="Number of variables with missing values per participant")
###


dat <- dat[, colMeans(is.na(dat)) <= threshold]
ncol(dat) # 52 columns left

columns_to_impute <- colnames(dat)[colSums(is.na(dat)) > 0]
columns_to_impute
str(dat[,columns_to_impute])

complete_columns <- colnames(dat)[colSums(is.na(dat)) == 0 & 
                                      !colnames(dat) %in% c('MICHD', 'CVDINFR4','CVDCRHD4')]

for (c in columns_to_impute) {
    col <- dat[[c]]
    scaled <- scale(dat[, complete_columns])
    knn <- knn(
        train = scaled[!is.na(col), complete_columns],
        test  = scaled[is.na(col), complete_columns], 
        cl    = dat[!is.na(col), c]
        )
    
    dat[is.na(col), c] = knn
}

colSums(is.na(dat))
```

```{r}
set.seed(263)
train_index <- createDataPartition(dat$MICHD, p = 0.8, list = FALSE)
train <- dat[train_index, ]
test <- dat[-train_index, ]
```

# XGBoost

In summary: 49 variables, 3 possible outcomes (MICHD, CVDINFR4, CVDCRHD4). N = 22,747 in training data, N = 5,686 in test data.

```{r}
train_variables <- train %>% 
  select(-MICHD, -CVDINFR4, -CVDCRHD4) 

train_variables_matrix <- train_variables %>% 
  data.matrix()
#train_outcomes <- train %>% 
#  select(MICHD, CVDINFR4, CVDCRHD4) %>% 
#  mutate(across(c(MICHD, CVDINFR4, CVDCRHD4), as.factor))

train_outcomes <- train %>%
  pull(MICHD) %>% 
  as.character() %>% 
  as.numeric()

test_variables <- test %>% 
  select(-MICHD, -CVDINFR4, -CVDCRHD4) 

test_variables_matrix <- test_variables%>% 
  data.matrix()

test_outcomes <- test %>% 
  pull(MICHD) %>% 
  as.character() %>% 
  as.numeric()

```

# XGBoost model with all 49 variables

```{r}
set.seed(43)

grid_tune <- expand.grid(
  nrounds = c(500,1000,1500), # Number of boosting rounds
  max_depth = c(1,3,5), # Max depth of tree
  eta = c(0.01, 0.1, 0.3), # Step size shrinkage to prevent overfitting
  gamma = 0, # Minimum loss reduction required to make a further partition on a leaf node of the tree
  colsample_bytree = 1, 
  min_child_weight = 1, 
  subsample = 1 
)

train_control <- trainControl(method = "cv", # Cross validation
                              number=3, # 3 folds
                              verboseIter = TRUE,
                              allowParallel = TRUE)

xgb_tune <- train(x = train_variables,
                  y = as.factor(train_outcomes),
                  trControl = train_control,
                  tuneGrid = grid_tune,
                  method= "xgbTree",
                  verbose = TRUE)
xgb_tune

```

```{r}
xgb_best <- xgb_tune$bestTune

train_control <- trainControl(method = "none",
                              verboseIter = TRUE,
                              allowParallel = TRUE)

final_grid <- expand.grid(nrounds = xgb_best$nrounds,
                           eta = xgb_best$eta,
                           max_depth = xgb_best$max_depth,
                           gamma = xgb_best$gamma,
                           colsample_bytree = xgb_best$colsample_bytree,
                           min_child_weight = xgb_best$min_child_weight,
                           subsample = xgb_best$subsample)
xgb_model <- train(x = train_variables,
                  y = as.factor(train_outcomes),
                   trControl = train_control,
                   tuneGrid = final_grid,
                   method = "xgbTree",
                   verbose = TRUE)
```

```{r}
xgb_pred <- predict(xgb_model, test_variables)

#' Confusion Matrix
confusionMatrix(as.factor((xgb_pred)),
                as.factor(test_outcomes))

library(pROC)
roc_score <- roc(test_outcomes, as.numeric(xgb_pred))
plot(roc_score, print.auc = T, main = "XGBoost with categorical labels")
```

# XGBoost with One-Hot encoding

49 variables become 220. 

Variables to remove: QSTVER, STSTR, RAWRAKE

```{r}
train_variables_2 <- train_variables %>% 
  mutate(across(c(-PHYSHLTH, -MENTHLTH, -CPDEMO1B, -STSTR, -RAWRAKE, -AGE80, -DROCDY3_), as.character))

test_variables_2 <- test_variables %>% 
  mutate(across(c(-PHYSHLTH, -MENTHLTH, -CPDEMO1B, -STSTR, -RAWRAKE, -AGE80, -DROCDY3_), as.character))
  
# Make one-hot encoded variables in the training set
dummy <- dummyVars(" ~ .", data=train_variables_2)
train_variables_3 <- data.frame(predict(dummy, newdata = train_variables_2)) %>% 
  select(-PRIMINSR6, -CHCCOPD39, -VETERAN33, -DEAF3, -BLIND3, -DIFFWALK3) # Remove 6 one hot encoded variables that do not exist in test dataset

# Make one-hot encoded variables in the test set
dummy <- dummyVars(" ~ .", data=test_variables_2)
test_variables_3 <- data.frame(predict(dummy, newdata = test_variables_2)) 

anti_join(as_tibble(colnames(train_variables_3)), as_tibble(colnames(test_variables_3)))
```

```{r}
set.seed(54)

grid_tune <- expand.grid(
  nrounds = c(500,1000,1500), # Number of boosting rounds
  max_depth = c(1,3,5), # Max depth of tree
  eta = c(0.01, 0.1, 0.3), # Step size shrinkage to prevent overfitting
  gamma = 0, # Minimum loss reduction required to make a further partition on a leaf node of the tree
  colsample_bytree = 1, 
  min_child_weight = 1, 
  subsample = 1 
)

train_control <- trainControl(method = "cv", # Cross validation
                              number=3, # 3 folds
                              verboseIter = TRUE,
                              allowParallel = TRUE)


xgb_tune_2 <- train(x = train_variables_3,
                  y = as.factor(train_outcomes),
                  trControl = train_control,
                  tuneGrid = grid_tune,
                  method= "xgbTree",
                  verbose = TRUE)
xgb_tune_2

```

```{r}
xgb_best_2 <- xgb_tune_2$bestTune
#Fitting nrounds = 1000, eta = 0.01, max_depth = 5, gamma = 0, colsample_bytree = 1, min_child_weight = 1, subsample = 1 on full training set

train_control <- trainControl(method = "none",
                              verboseIter = TRUE,
                              allowParallel = TRUE)

final_grid_2 <- expand.grid(nrounds = xgb_best_2$nrounds,
                           eta = xgb_best_2$eta,
                           max_depth = xgb_best_2$max_depth,
                           gamma = xgb_best_2$gamma,
                           colsample_bytree = xgb_best_2$colsample_bytree,
                           min_child_weight = xgb_best_2$min_child_weight,
                           subsample = xgb_best_2$subsample)

xgb_model_2 <- train(x = train_variables_3,
                  y = as.factor(train_outcomes),
                   trControl = train_control,
                   tuneGrid = final_grid_2,
                   method = "xgbTree",
                   verbose = TRUE)
```

```{r}
xgb_pred_2 <- predict(xgb_model_2, test_variables_3)

confusionMatrix(as.factor((xgb_pred_2)),
                as.factor(test_outcomes))

roc_score_2 <- roc(test_outcomes, as.numeric(xgb_pred_2))
plot(roc_score_2, print.auc = T, main = "XGBoost with one-hot encoding")
```


